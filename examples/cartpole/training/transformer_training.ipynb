{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer ILQR Training\n",
    "\n",
    "This notebook demonstrates the step-by-step training process of the Transformer ILQR model. The notebook:\n",
    "\n",
    "- Loads ILQR log data from a pickle file\n",
    "- Processes the data into a pandas DataFrame\n",
    "- Splits the data into training and testing sets\n",
    "- Trains and saves the Transformer model using the training set\n",
    "\n",
    "After training, the loss history is available for external plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module path: /Users/justin/PycharmProjects/quattro-transformer-ilqr/examples/cartpole/training\n",
      "Imports successful.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Import required functions from transforerm_training.py\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "print(f\"Module path: {module_path}\")\n",
    "\n",
    "from transformer_training import load_ilqr_logs, process_ilqr_logs, split_data, train_transformer\n",
    "\n",
    "print('Imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load ILQR Logs\n",
    "Specify the path to your ILQR logs pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ILQR logs from combined_ilqr_logs_range_-0.500_0.500_angle_-0.500_0.500.pkl...\n",
      "Loaded 14694 log entries.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the combined ILQR logs file\n",
    "log_file_path = \"combined_ilqr_logs_range_-0.500_0.500_angle_-0.500_0.500.pkl\"\n",
    "\n",
    "ilqr_logs = load_ilqr_logs(log_file_path)\n",
    "if ilqr_logs is None:\n",
    "    raise ValueError('Failed to load ILQR logs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process ILQR Logs\n",
    "Convert the raw log entries into a pandas DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_ilqr_logs(ilqr_logs)\n",
    "print('Sample data from logs:')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split the Data\n",
    "Shuffle and split the DataFrame into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11755\n",
      "Test set size: 2939\n",
      "Training set size: 11755\n",
      "Test set size: 2939\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = split_data(df, train_fraction=0.8, random_state=42)\n",
    "\n",
    "print('Training set size:', len(train_df))\n",
    "print('Test set size:', len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Save the Transformer Model\n",
    "Train the Transformer ILQR model using the training DataFrame. You can adjust hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Loaded. Device is: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 11755/11755 [00:00<00:00, 50134.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerPredictor(\n",
      "  (state_embed): Linear(in_features=4, out_features=256, bias=True)\n",
      "  (control_embed): Linear(in_features=5, out_features=256, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output_linear): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (transformer_decoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 0.430058\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Set training hyperparameters\n",
    "num_epochs = 1       # Adjust number of epochs\n",
    "batch_size = 128     \n",
    "learning_rate = 1e-4 \n",
    "prompt_len = 1    \n",
    "d_model = 256        \n",
    "nhead = 8            \n",
    "\n",
    "model_wrapper = train_transformer(train_df,\n",
    "                                  num_epochs=num_epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  prompt_len=prompt_len,\n",
    "                                  d_model=d_model,\n",
    "                                  nhead=nhead)\n",
    "\n",
    "print('Training completed.')\n",
    "\n",
    "# Save the trained model\n",
    "model_wrapper.save(\"cartpole\")\n",
    "print('Model saved successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss History\n",
    "The training loss history is stored in `model_wrapper.train_loss_history`.\n",
    "You can plot these loss curves externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss history: [0.4300576684754841]\n"
     ]
    }
   ],
   "source": [
    "print('Train loss history:', model_wrapper.train_loss_history)\n",
    "if hasattr(model_wrapper, 'test_loss_history'):\n",
    "    print('Test loss history:', model_wrapper.test_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
